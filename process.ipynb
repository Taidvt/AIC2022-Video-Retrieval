{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import faiss\n",
    "import Levenshtein\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import open_clip\n",
    "from open_clip import tokenizer\n",
    "from flask import Flask, request, jsonify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "def setup_app(app):\n",
    "    # All your initialization code\n",
    "    print('Initialize Server')\n",
    "\n",
    "    #For CLIP\n",
    "    with open( '/mmlabworkspace/Students/AIC/ALL_3batch_CLIPFeatures.pkl', \"rb\") as f:\n",
    "      image_features = pickle.load(f)\n",
    "      \n",
    "    model, _, preprocess = open_clip.create_model_and_transforms('ViT-B-16', pretrained='openai')\n",
    "    model.eval()\n",
    "\n",
    "    # Load translate model\n",
    "    # model_name = \"VietAI/envit5-translation\"\n",
    "    # tokenizer_vn = AutoTokenizer.from_pretrained(model_name)\n",
    "    # translate_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).cuda()\n",
    "\n",
    "\n",
    "    #context_length = model.context_length\n",
    "    #vocab_size = model.vocab_size\n",
    "    \n",
    "    feature_shape = 512\n",
    "    res = faiss.StandardGpuResources()\n",
    "    flat_config = faiss.GpuIndexFlatConfig()\n",
    "    flat_config.device = 0\n",
    "    #index = faiss.IndexFlatL2(512)\n",
    "    index = faiss.GpuIndexFlatL2(res, feature_shape, flat_config)\n",
    "    index.add(image_features)\n",
    "    k = 1000\n",
    "    #k=20\n",
    "    \n",
    "    #For OCR\n",
    "    f = open(\"/mmlabworkspace/Students/AIC/ALL_3batch_OCR_Metadata.json\")\n",
    "    data_ocr = json.load(f)\n",
    "    f.close()\n",
    "    return model, index, data_ocr, k, #tokenizer_vn, translate_model\n",
    "    \n",
    "# model, index, data_ocr, k, tokenizer_vn, translate_model = setup_app(app)\n",
    "model, index, data_ocr, k, = setup_app(app)\n",
    "\n",
    "# def translateVi2En(input_sentence):\n",
    "#   return tokenizer_vn.batch_decode(translate_model.generate(tokenizer_vn(input_sentence, return_tensors=\"pt\", padding=True).input_ids.cuda(), max_length=128), skip_special_tokens=True)[0][3:]\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@app.route(\"/process\", methods = [\"POST\",\"GET\"])\n",
    "def result():\n",
    "  # query_input = request.data  # OCR input\n",
    "  query = request.json[\"query\"]\n",
    "  # query = translateVi2En(query)\n",
    "  print(query)\n",
    "\n",
    "  mode = request.json[\"mode\"]\n",
    "  mode_ocr = \"visual\"\n",
    "  score = 0.85\n",
    "  \n",
    "  if mode == \"ocr\":\n",
    "    indeces = []\n",
    "    query = query.split('&')\n",
    "    for i in data_ocr:\n",
    "      true_q = 0\n",
    "      for j in data_ocr[i]['words']:    \n",
    "          for num_q in query:\n",
    "              #if this is visual mode, we have to check whether the query is greater than score or not\n",
    "              if mode_ocr == \"visual\":\n",
    "                  if num_q[0] == '\"':\n",
    "                      num_q = num_q[1:-1]\n",
    "                      #if (Levenshtein.ratio(num_q, j)) == 1:\n",
    "                      if num_q == j: \n",
    "                          true_q += 1\n",
    "                  else:\n",
    "                      if ((Levenshtein.ratio(num_q, j)) >= score) or ( num_q in j):\n",
    "                          true_q += 1\n",
    "              \n",
    "              # if this is textual mode, we just check whether query is in description or not\n",
    "              # elif mode_ocr == \"textual\":\n",
    "              #     if num_q in j:\n",
    "              #         true_q += 1\n",
    "      \n",
    "      # In the visual mode case, if true_q == len(query) => it's the index of keyframe that we seek\n",
    "      # In the textual mode case, if true_q >= len(query), it means that the query in the description equal or more than len(query) times\n",
    "      if true_q >= len(query):    \n",
    "          indeces.append(i)\n",
    "          \n",
    "    return json.dumps(indeces)\n",
    "  \n",
    "  elif mode == \"caption\":\n",
    "    text_tokens = tokenizer.tokenize([query])\n",
    "    with torch.no_grad():\n",
    "      text_features = model.encode_text(text_tokens).float()\n",
    "      text_features /= text_features.norm(dim=-1, keepdim = True)\n",
    "      text_features = text_features.cpu().numpy()\n",
    "      \n",
    "    D,I = index.search(text_features, k)\n",
    "    \n",
    "    I = I.tolist()\n",
    "    I = I[0]\n",
    "    print(\"I\", I)\n",
    "    return json.dumps(I)\n",
    "\n",
    "\n",
    "# @app.route(\"/process_FAISS\", methods = [\"POST\",\"GET\"])\n",
    "# def result():\n",
    "#     query = request.json[\"query\"]\n",
    "#     print(query)\n",
    "    \n",
    "#     data = {\"id\": \"1\", \"title\": \"2\"}\n",
    "#     resp = jsonify(data)\n",
    "#     resp.status_code = 200\n",
    "#     print(resp)\n",
    "#     return resp \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host=\"0.0.0.0\", debug=False, port=2021,threaded=True)\n",
    "    # print(\"here\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
